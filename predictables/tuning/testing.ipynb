{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aweaver/work/hit-ratio-model\n"
     ]
    }
   ],
   "source": [
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PredicTables.util import code_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_.copy_folder_code(\"./PredicTables/model/opt/src/bayes/\", \"py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from typing import Callable, Any\n",
    "\n",
    "\n",
    "def parallel_optimization(\n",
    "    objective_func: Callable[..., Any],\n",
    "    space: skopt.Space,\n",
    "    n_calls: int,\n",
    "    batch_size: int = 4,\n",
    "    backend: str = \"multiprocessing\",\n",
    ") -> skopt.Optimizer:\n",
    "    \"\"\"\n",
    "    Run Bayesian optimization in parallel. The objective function is evaluated\n",
    "    in parallel using either a process pool or a thread pool.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    objective_func : callable\n",
    "        Objective function to be optimized.\n",
    "    space : skopt.Space\n",
    "        Search space.\n",
    "    n_calls : int\n",
    "        Number of calls to `objective_func`.\n",
    "    batch_size : int, optional\n",
    "        Number of points to evaluate in parallel. Defaults to 4.\n",
    "    backend : str, optional\n",
    "        Parallelization backend. Either \"multiprocessing\" or \"threading\".\n",
    "        Defaults to \"multiprocessing\". Note that \"threading\" is only useful\n",
    "        when `objective_func` is not CPU-bound. This is rarely the case in\n",
    "        hyperparameter optimization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    optimizer : skopt.Optimizer\n",
    "        Optimizer instance.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import skopt\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.datasets import load_cancer\n",
    "    >>> from sklearn.svm import SVC\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> from sklearn.preprocessing import StandardScaler\n",
    "    >>> from sklearn.decomposition import PCA\n",
    "    >>> from sklearn.model_selection import train_test_split\n",
    "    >>> from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "    >>> X, y = load_cancer(return_X_y=True)\n",
    "    >>> X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    >>> def objective_func(params):\n",
    "    ...     C, gamma = params\n",
    "    ...     model = Pipeline([\n",
    "    ...         (\"scaler\", StandardScaler()),\n",
    "    ...         (\"pca\", PCA(n_components=2)),\n",
    "    ...         (\"svm\", SVC(C=C, gamma=gamma, random_state=0, probability=True)),\n",
    "    ...     ])\n",
    "    ...     score = cross_val_score(\n",
    "    ...         model, X_train, y_train, cv=3, scoring=make_scorer(roc_auc_score)\n",
    "    ...     ).mean()\n",
    "    ...     return 1 - score\n",
    "\n",
    "    >>> space = [\n",
    "    ...     skopt.space.Real(1e-6, 1e+6, prior=\"log-uniform\", name=\"C\"),\n",
    "    ...     skopt.space.Real(1e-6, 1e+1, prior=\"log-uniform\", name=\"gamma\"),\n",
    "    ... ]\n",
    "\n",
    "    >>> optimizer = parallel_optimization(\n",
    "    ...     objective_func, space, n_calls=20, batch_size=4, backend=\"multiprocessing\"\n",
    "    ... )\n",
    "    >>> best_params = optimizer.x\n",
    "    >>> best_score = optimizer.fun\n",
    "    >>> best_params, best_score\n",
    "    ([0.0001, 0.0001], 0.012...)\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    skopt.Optimizer : Optimizer class.\n",
    "    skopt.space : Search space.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function is a simplified version of the `parallel_optimization`\n",
    "    function from the `skopt` library. It is meant to be used as a drop-in\n",
    "    replacement for `skopt.gp_minimize` and `skopt.forest_minimize` when\n",
    "    running on a cluster. The `objective_func` is evaluated in parallel using\n",
    "    either a process pool or a thread pool. The `batch_size` parameter\n",
    "    controls the number of points to evaluate in parallel. The `backend`\n",
    "    parameter controls the parallelization backend. Either \"multiprocessing\"\n",
    "    or \"threading\". Note that \"threading\" is only useful when `objective_func`\n",
    "    is not CPU-bound. This is rarely the case in hyperparameter optimization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html\n",
    "    https://scikit-optimize.github.io/stable/modules/generated/skopt.forest_minimize.html\n",
    "    https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html\n",
    "    https://scikit-optimize.github.io/stable/modules/generated/skopt.space.html\n",
    "    \"\"\"\n",
    "    if backend == \"multiprocessing\":\n",
    "        return _parallel_optimization_process_pool(\n",
    "            objective_func, space, n_calls, batch_size\n",
    "        )\n",
    "    elif backend == \"threading\":\n",
    "        return _parallel_optimization_thread_pool(\n",
    "            objective_func, space, n_calls, batch_size\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unknown backend: {backend}. \"\n",
    "            f\"Please choose between 'multiprocessing' and 'threading'.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _parallel_optimization_process_pool(\n",
    "    objective_func: Callable[..., Any],\n",
    "    space: skopt.Space,\n",
    "    n_calls: int,\n",
    "    batch_size: int,\n",
    ") -> skopt.Optimizer:\n",
    "    \"\"\"\n",
    "    Internal function to run Bayesian optimization in parallel using a process\n",
    "    pool. Mainly used by the `parallel_optimization` function. Not meant to be\n",
    "    used directly.\n",
    "    \"\"\"\n",
    "    from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "    optimizer = skopt.Optimizer(space)\n",
    "    for _ in range(n_calls // batch_size):\n",
    "        points = [optimizer.ask() for _ in range(batch_size)]\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            evaluations = list(executor.map(objective_func, points))\n",
    "\n",
    "        for point, evaluation in zip(points, evaluations):\n",
    "            optimizer.tell(point, evaluation)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def _parallel_optimization_thread_pool(\n",
    "    objective_func: Callable[..., Any],\n",
    "    space: skopt.Space,\n",
    "    n_calls: int,\n",
    "    batch_size: int,\n",
    ") -> skopt.Optimizer:\n",
    "    \"\"\"\n",
    "    Internal function to run Bayesian optimization in parallel using a thread\n",
    "    pool. Mainly used by the `parallel_optimization` function. Not meant to be\n",
    "    used directly.\n",
    "    \"\"\"\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "    optimizer = skopt.Optimizer(space)\n",
    "    for _ in range(n_calls // batch_size):\n",
    "        # Ask for a batch of points\n",
    "        points = [optimizer.ask() for _ in range(batch_size)]\n",
    "\n",
    "        # Evaluate points in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            evaluations = list(executor.map(objective_func, points))\n",
    "\n",
    "        # Update the model with the results\n",
    "        for point, evaluation in zip(points, evaluations):\n",
    "            optimizer.tell(point, evaluation)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "from skopt.acquisition import gaussian_ei, gaussian_pi, gaussian_lcb\n",
    "\n",
    "\n",
    "def dynamic_ucb(optimizer, x, batch_size=4):\n",
    "    \"\"\"\n",
    "    Uses the dynamic upper confidence bound acquisition function to determine\n",
    "    the next point to evaluate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : skopt.Optimizer\n",
    "        The optimizer object.\n",
    "    x : array-like\n",
    "        The point to evaluate.\n",
    "    batch_size : int\n",
    "        The number of points to evaluate in parallel. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_point : array-like\n",
    "        The next point to evaluate.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The dynamic upper confidence bound acquisition function is defined as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "            UCB(x) = \\mu(x) + \\beta \\sigma(x)\n",
    "\n",
    "        where :math:`\\mu(x)` is the mean of the surrogate model, and\n",
    "        :math:`\\sigma(x)` is the standard deviation of the surrogate model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the dynamic upper confidence bound acquisition function\n",
    "    def acq_func(x):\n",
    "        return -optimizer._gp.predict(x.reshape(1, -1), return_std=True)[1]\n",
    "\n",
    "    # Ask for the next point to evaluate\n",
    "    return optimizer.ask(acq_func=acq_func, n_points=batch_size)\n",
    "\n",
    "\n",
    "def expected_improvement(optimizer, x, batch_size=4):\n",
    "    \"\"\"\n",
    "    Uses the expected improvement acquisition function to determine the next\n",
    "    point to evaluate. This is the default acquisition function used by\n",
    "    scikit-optimize.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : skopt.Optimizer\n",
    "        The optimizer object.\n",
    "    x : array-like\n",
    "        The point to evaluate.\n",
    "    batch_size : int\n",
    "        The number of points to evaluate in parallel. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_point : array-like\n",
    "        The next point to evaluate.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The expected improvement acquisition function is defined as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "            EI(x) = \\mathbb{E} \\left[ \\max(0, f(x) - f(x^+)) \\right]\n",
    "\n",
    "        where :math:`f` is the surrogate model, and :math:`x^+` is the best point\n",
    "\n",
    "    EI(x) tends to favor points that are close to the current best point, but\n",
    "    with high uncertainty. This encourages exploration near the current best\n",
    "    point, but also exploitation of points that are likely to be better than\n",
    "    the current best point.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the expected improvement acquisition function\n",
    "    def acq_func(x):\n",
    "        return -gaussian_ei(x.reshape(1, -1), optimizer._gp)\n",
    "\n",
    "    # Ask for the next point to evaluate\n",
    "    return optimizer.ask(acq_func=acq_func, n_points=batch_size)\n",
    "\n",
    "\n",
    "def probability_of_improvement(optimizer, x, xi=0.01, batch_size=4):\n",
    "    \"\"\"\n",
    "    Uses the probability of improvement acquisition function to determine the\n",
    "    next point to evaluate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : skopt.Optimizer\n",
    "        The optimizer object.\n",
    "    x : array-like\n",
    "        The point to evaluate.\n",
    "    xi : float\n",
    "        Controls the amount of exploration. Defaults to 0.01.\n",
    "    batch_size : int\n",
    "        The number of points to evaluate in parallel. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_point : array-like\n",
    "        The next point to evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the probability of improvement acquisition function\n",
    "    def acq_func(x):\n",
    "        return -gaussian_pi(x.reshape(1, -1), optimizer._gp, xi=xi)\n",
    "\n",
    "    # Ask for the next point to evaluate\n",
    "    return optimizer.ask(acq_func=acq_func, n_points=batch_size)\n",
    "\n",
    "\n",
    "def lower_confidence_bound(optimizer, x, kappa=1.96, batch_size=4):\n",
    "    \"\"\"\n",
    "    Uses the lower confidence bound acquisition function to determine the next\n",
    "    point to evaluate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : skopt.Optimizer\n",
    "        The optimizer object.\n",
    "    x : array-like\n",
    "        The point to evaluate.\n",
    "    kappa : float\n",
    "        Controls the amount of exploration. Defaults to 1.96.\n",
    "    batch_size : int\n",
    "        The number of points to evaluate in parallel. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_point : array-like\n",
    "        The next point to evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the lower confidence bound acquisition function\n",
    "    def acq_func(x):\n",
    "        return -gaussian_lcb(x.reshape(1, -1), optimizer._gp, kappa=kappa)\n",
    "\n",
    "    # Ask for the next point to evaluate\n",
    "    return optimizer.ask(acq_func=acq_func, n_points=batch_size)\n",
    "\n",
    "\n",
    "from .parallel_optimization import parallel_optimization\n",
    "from .objective_function import objective_with_pruning, _objective_function_no_pruning\n",
    "from .get_space import get_space\n",
    "from .get_model_obj import get_model_obj\n",
    "\n",
    "from PredicTables.util import model_name_map\n",
    "\n",
    "\n",
    "def get_model_obj(model_name: str, task_type: str = \"classification\", **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a newly-instantiated model object. Accepts keyword arguments to be\n",
    "    passed to the model constructor.\n",
    "    \"\"\"\n",
    "    # Check and standardize inputs\n",
    "    model_name = model_name_map(model_name)\n",
    "\n",
    "    task_type = task_type.lower().strip()\n",
    "    if task_type not in [\"classification\", \"regression\"]:\n",
    "        raise ValueError(\n",
    "            f'Invalid task_type {task_type}. Must be one of \"classification\" or \"regression\".'\n",
    "        )\n",
    "\n",
    "    if model_name == \"catboost\":\n",
    "        if task_type == \"classification\":\n",
    "            from catboost import CatBoostClassifier\n",
    "\n",
    "            return CatBoostClassifier(**kwargs)\n",
    "        elif task_type == \"regression\":\n",
    "            from catboost import CatBoostRegressor\n",
    "\n",
    "            return CatBoostRegressor(**kwargs)\n",
    "\n",
    "    elif model_name == \"random_forest\":\n",
    "        if task_type == \"classification\":\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "            return RandomForestClassifier(**kwargs)\n",
    "        elif task_type == \"regression\":\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "            return RandomForestRegressor(**kwargs)\n",
    "\n",
    "    elif model_name == \"elastic_net\":\n",
    "        from sklearn.linear_model import ElasticNet\n",
    "\n",
    "        return ElasticNet(**kwargs)\n",
    "\n",
    "    elif model_name == \"svm\":\n",
    "        if task_type == \"classification\":\n",
    "            from sklearn.svm import SVC\n",
    "\n",
    "            return SVC(**kwargs)\n",
    "        elif task_type == \"regression\":\n",
    "            from sklearn.svm import SVR\n",
    "\n",
    "            return SVR(**kwargs)\n",
    "\n",
    "    elif model_name == \"lightgbm\":\n",
    "        if task_type == \"classification\":\n",
    "            from lightgbm import LGBMClassifier\n",
    "\n",
    "            return LGBMClassifier(**kwargs)\n",
    "        elif task_type == \"regression\":\n",
    "            from lightgbm import LGBMRegressor\n",
    "\n",
    "            return LGBMRegressor(**kwargs)\n",
    "\n",
    "    elif model_name == \"xgboost\":\n",
    "        if task_type == \"classification\":\n",
    "            from xgboost import XGBClassifier\n",
    "\n",
    "            return XGBClassifier(**kwargs)\n",
    "        elif task_type == \"regression\":\n",
    "            from xgboost import XGBRegressor\n",
    "\n",
    "            return XGBRegressor(**kwargs)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid model name {model_name}. Must be one of \"\n",
    "            f\"'catboost', 'random_forest', 'elastic_net', 'svm', 'lightgbm', \"\n",
    "            f\"or 'xgboost'.\"\n",
    "        )\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.base import BaseEstimator\n",
    "from typing import Union\n",
    "from PredicTables.util import to_pd_df, to_pd_s\n",
    "\n",
    "\n",
    "def _objective_function_no_pruning(params):\n",
    "    \"\"\"\n",
    "    This is for testing purposes only. It is not used in the actual\n",
    "    optimization process. It is a simple objective function that seeks\n",
    "    to minimize the L1 norm of the parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        Hyperparameters for the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The sum of the absolute values of the parameters.\n",
    "    \"\"\"\n",
    "    # Absolute value of the parameters\n",
    "    abs_params = np.abs(list(params.values()))\n",
    "\n",
    "    # Return the sum of the absolute values of the parameters\n",
    "    return np.sum(abs_params)\n",
    "\n",
    "\n",
    "def objective_with_pruning(\n",
    "    params: dict,\n",
    "    model: BaseEstimator,\n",
    "    X_train: Union[pd.DataFrame, pl.DataFrame, pl.LazyFrame],\n",
    "    y_train: Union[pd.Series, pl.Series],\n",
    "    X_val: Union[pd.DataFrame, pl.DataFrame, pl.LazyFrame],\n",
    "    y_val: Union[pd.Series, pl.Series],\n",
    "    pruning_threshold: float,\n",
    "    n_checkpoints: int = 10,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Objective function that includes logic for pruning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        Hyperparameters for the model.\n",
    "    model : sklearn.base.BaseEstimator\n",
    "        Machine learning model that supports partial fitting or early stopping,\n",
    "        typically inheriting from sklearn.base.BaseEstimator.\n",
    "    X_train, y_train : array-like\n",
    "        Training features and labels.\n",
    "    X_val, y_val : array-like\n",
    "        Validation features and labels. Used for pruning.\n",
    "    pruning_threshold : float\n",
    "        Threshold for pruning. If the model's performance on the validation set\n",
    "        is worse than this threshold, the model is pruned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The model's performance on the validation set.\n",
    "    \"\"\"\n",
    "    # Convert to pandas\n",
    "    X_train = to_pd_df(X_train)\n",
    "    y_train = to_pd_s(y_train)\n",
    "    X_val = to_pd_df(X_val)\n",
    "    y_val = to_pd_s(y_val)\n",
    "\n",
    "    # Initialize the best score to infinity\n",
    "    best_evaluation = float(\"inf\")\n",
    "\n",
    "    # Loop over checkpoints, fitting the model at each checkpoint, evaluating,\n",
    "    # and making sure the improvement justifies additional training time\n",
    "    # (i.e., pruning)\n",
    "    pct_to_train = 1.0 / n_checkpoints\n",
    "    for checkpoint in range(1, n_checkpoints + 1):\n",
    "        # Calculate the number of iterations to train for at this checkpoint\n",
    "        pct_trained = pct_to_train * checkpoint\n",
    "        n_iterations = int(pct_trained * params[\"n_iterations\"])\n",
    "\n",
    "        # Update the model with the new number of iterations\n",
    "        model = partial_fit(X_train, y_train, model, n_iterations=n_iterations)\n",
    "\n",
    "        # Evaluate the model\n",
    "        current_evaluation = evaluate_partial_fit(X_val, y_val, model)\n",
    "\n",
    "        # Pruning condition: if current score is worse than the threshold, prune\n",
    "        if should_model_be_pruned(\n",
    "            current_evaluation, best_evaluation, pruning_threshold\n",
    "        ):\n",
    "            break\n",
    "\n",
    "        # If not pruned, update the best score\n",
    "        best_evaluation = current_evaluation\n",
    "\n",
    "    # By exiting the loop, we have either pruned the model or trained it to\n",
    "    # completion. If the model was pruned, we need to retrain it to completion\n",
    "    # before returning the final evaluation.\n",
    "    model = complete_fit(X_train, y_train, model, **params)\n",
    "\n",
    "    # Evaluate the model\n",
    "    final_evaluation = evaluate_model(X_val, y_val, model)\n",
    "\n",
    "    # Return the best evaluation if the model was pruned, otherwise return\n",
    "    # the final evaluation\n",
    "    final_score = (\n",
    "        best_evaluation if best_evaluation < final_evaluation else final_evaluation\n",
    "    )\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "def partial_fit(X_train, y_train, model):\n",
    "    # Partially fit the model here\n",
    "    # Depending on the model, different approaches may be needed\n",
    "    # such as early stopping, training for a certain number of iterations, etc.\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate_partial_fit(X_val, y_val, model):\n",
    "    # Evaluate the model here\n",
    "    # Evaluate consistently with ultimate goal of optimization process --\n",
    "    # e.g., if optimizing for accuracy, evaluate using accuracy, not AUC\n",
    "    pass\n",
    "\n",
    "\n",
    "def should_model_be_pruned(new_score, current_score, pruning_factor=1, cond=\"gt\"):\n",
    "    # Define pruning condition here\n",
    "    # implementing a simple one for now\n",
    "    if cond == \"gt\":\n",
    "        return current_score > (pruning_factor * new_score)\n",
    "    elif cond == \"lt\":\n",
    "        return current_score < (pruning_factor * new_score)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Condition {cond} not implemented. Currently only 'gt' and 'lt' are supported.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def complete_fit(X_train, y_train, model, **params):\n",
    "    # Fit the model here\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate_model(X_val, y_val, model):\n",
    "    # Evaluate the model here\n",
    "    # Evaluate consistently with ultimate goal of optimization process --\n",
    "    # e.g., if optimizing for accuracy, evaluate using accuracy, not AUC\n",
    "    pass\n",
    "\n",
    "\n",
    "def adjust_exploration_parameter(iteration, max_iterations, performance_metrics=None):\n",
    "    # Adjust exploration parameter based on iteration or performance metrics\n",
    "    # Implement a conservative strategy to minimize missing global optimum\n",
    "    pass\n",
    "\n",
    "\n",
    "import skopt\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from typing import List\n",
    "\n",
    "from PredicTables.util import model_name_map\n",
    "\n",
    "\n",
    "def get_space(model_name: str, **kwargs) -> List[skopt.Space]:\n",
    "    \"\"\"\n",
    "    Returns a properly-formatted space for hyperparameter optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Name of the model to optimize. Must be one of \"catboost\", \"random_forest\",\n",
    "        \"elastic_net\", \"svm\", \"lightgbm\", or \"xgboost\".\n",
    "    kwargs : dict\n",
    "        Keyword arguments to be added to the default space, or to use to replace\n",
    "        default space values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    space : list of skopt.space objects\n",
    "        A list of skopt.space objects.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from PredicTables.model.opt import Bayes\n",
    "    >>> space = Bayes.get_space(\"catboost\")\n",
    "    >>> space\n",
    "    [Integer(low=1, high=15, prior='uniform', transform='identity'),\n",
    "     Real(low=0.01, high=1.0, prior='uniform', transform='identity'),\n",
    "     Integer(low=1, high=10, prior='uniform', transform='identity'),\n",
    "     Categorical(categories=('SymmetricTree', 'Depthwise', 'Lossguide'), prior=None),\n",
    "     Integer(low=1, high=255, prior='uniform', transform='identity'),\n",
    "     Real(low=0.0, high=1.0, prior='uniform', transform='identity')]\n",
    "\n",
    "    >>> space = Bayes.get_space(\"catboost\", learning_rate=Real(0.5, 1.0))\n",
    "    >>> space\n",
    "    [Integer(low=1, high=15, prior='uniform', transform='identity'),\n",
    "     Real(low=0.5, high=1.0, prior='uniform', transform='identity'),\n",
    "     Integer(low=1, high=10, prior='uniform', transform='identity'),\n",
    "     Categorical(categories=('SymmetricTree', 'Depthwise', 'Lossguide'), prior=None),\n",
    "     Integer(low=1, high=255, prior='uniform', transform='identity'),\n",
    "     Real(low=0.0, high=1.0, prior='uniform', transform='identity')]\n",
    "    \"\"\"\n",
    "    if model_name_map(model_name) == \"catboost\":\n",
    "        return get_catboost_space(**kwargs)\n",
    "    elif model_name_map(model_name) == \"random_forest\":\n",
    "        return get_random_forest_space(**kwargs)\n",
    "    elif model_name_map(model_name) == \"elastic_net\":\n",
    "        return get_elastic_net_space(**kwargs)\n",
    "    elif model_name_map(model_name) == \"svm\":\n",
    "        return get_svm_space(**kwargs)\n",
    "    elif model_name_map(model_name) == \"lightgbm\":\n",
    "        return get_lightgbm_space(**kwargs)\n",
    "    elif model_name_map(model_name) == \"xgboost\":\n",
    "        return get_xgboost_space(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid model name {model_name}. Must be one of \"\n",
    "            f\"'catboost', 'random_forest', 'elastic_net', 'svm', 'lightgbm', \"\n",
    "            f\"or 'xgboost'.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_catboost_space(**kwargs) -> List[skopt.Space]:\n",
    "    \"\"\"\n",
    "    Returns a properly-formatted space for CatBoost hyperparameter optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kwargs : dict\n",
    "        Keyword arguments to be added to the default space, or to use to replace\n",
    "        default space values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    space : list of skopt.space objects\n",
    "        A list of skopt.space objects.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from PredicTables.model.opt import Bayes\n",
    "    >>> space = Bayes.get_catboost_space()\n",
    "    >>> space\n",
    "    [Integer(low=1, high=15, prior='uniform', transform='identity'),\n",
    "     Real(low=0.01, high=1.0, prior='uniform', transform='identity'),\n",
    "     Integer(low=1, high=10, prior='uniform', transform='identity'),\n",
    "     Categorical(categories=('SymmetricTree', 'Depthwise', 'Lossguide'), prior=None),\n",
    "     Integer(low=1, high=255, prior='uniform', transform='identity'),\n",
    "     Real(low=0.0, high=1.0, prior='uniform', transform='identity')]\n",
    "\n",
    "    >>> space = Bayes.get_catboost_space(learning_rate=Real(0.5, 1.0))\n",
    "    >>> space\n",
    "    [Integer(low=1, high=15, prior='uniform', transform='identity'),\n",
    "     Real(low=0.5, high=1.0, prior='uniform', transform='identity'),\n",
    "     Integer(low=1, high=10, prior='uniform', transform='identity'),\n",
    "     Categorical(categories=('SymmetricTree', 'Depthwise', 'Lossguide'), prior=None),\n",
    "     Integer(low=1, high=255, prior='uniform', transform='identity'),\n",
    "     Real(low=0.0, high=1.0, prior='uniform', transform='identity')]\n",
    "    \"\"\"\n",
    "    # Define default space\n",
    "    default_space = {\n",
    "        \"depth\": Integer(1, 15),\n",
    "        \"learning_rate\": Real(0.01, 1.0),\n",
    "        \"l2_leaf_reg\": Integer(1, 10),\n",
    "        \"grow_policy\": Categorical([\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]),\n",
    "        \"border_count\": Integer(1, 255),\n",
    "        \"bagging_temperature\": Real(0.0, 1.0),\n",
    "    }\n",
    "\n",
    "    # Update default space with kwargs\n",
    "    updated_space = {**default_space, **kwargs}\n",
    "\n",
    "    # Convert to skopt space format\n",
    "    return [updated_space[key] for key in updated_space]\n",
    "\n",
    "\n",
    "def get_random_forest_space(**kwargs):\n",
    "    # Define default space\n",
    "    default_space = [\n",
    "        Integer(10, 1000, name=\"n_estimators\"),\n",
    "        Integer(1, 30, name=\"max_depth\"),\n",
    "        Real(0.1, 0.9, name=\"min_samples_split\"),\n",
    "        Integer(1, 60, name=\"min_samples_leaf\"),\n",
    "        Categorical([\"auto\", \"sqrt\", \"log2\"], name=\"max_features\"),\n",
    "        Categorical([True, False], name=\"bootstrap\"),\n",
    "    ]\n",
    "\n",
    "    # Update default space with kwargs\n",
    "    updated_space = {**default_space, **kwargs}\n",
    "\n",
    "    # Convert to skopt space format\n",
    "    return [updated_space[key] for key in updated_space]\n",
    "\n",
    "\n",
    "def get_elastic_net_space(**kwargs):\n",
    "    # Define default space\n",
    "    default_space = [Real(0.0, 1.0, name=\"alpha\"), Real(0.0, 1.0, name=\"l1_ratio\")]\n",
    "\n",
    "    # Update default space with kwargs\n",
    "    updated_space = {**default_space, **kwargs}\n",
    "\n",
    "    # Convert to skopt space format\n",
    "    return [updated_space[key] for key in updated_space]\n",
    "\n",
    "\n",
    "def get_svm_space(**kwargs):\n",
    "    # Define default space\n",
    "    default_space = [\n",
    "        Real(1e-6, 1e6, name=\"C\", prior=\"log-uniform\"),\n",
    "        Categorical([\"linear\", \"poly\", \"rbf\", \"sigmoid\"], name=\"kernel\"),\n",
    "        Real(1e-6, 1e2, name=\"gamma\", prior=\"log-uniform\"),\n",
    "        Integer(1, 5, name=\"degree\"),\n",
    "    ]\n",
    "\n",
    "    # Update default space with kwargs\n",
    "    updated_space = {**default_space, **kwargs}\n",
    "\n",
    "    # Convert to skopt space format\n",
    "    return [updated_space[key] for key in updated_space]\n",
    "\n",
    "\n",
    "def get_lightgbm_space(**kwargs):\n",
    "    # Define default space\n",
    "    default_space = [\n",
    "        Integer(1, 15, name=\"num_leaves\"),\n",
    "        Real(0.01, 1.0, name=\"learning_rate\"),\n",
    "        Integer(1, 100, name=\"min_data_in_leaf\"),\n",
    "        Real(0.01, 1.0, name=\"feature_fraction\"),\n",
    "        Real(0.01, 1.0, name=\"bagging_fraction\"),\n",
    "        Integer(1, 7, name=\"bagging_freq\"),\n",
    "        Integer(1, 255, name=\"max_bin\"),\n",
    "    ]\n",
    "\n",
    "    # Update default space with kwargs\n",
    "    updated_space = {**default_space, **kwargs}\n",
    "\n",
    "    # Convert to skopt space format\n",
    "    return [updated_space[key] for key in updated_space]\n",
    "\n",
    "\n",
    "def get_xgboost_space(**kwargs):\n",
    "    # Define default space\n",
    "    default_space = [\n",
    "        Integer(1, 15, name=\"max_depth\"),\n",
    "        Real(0.01, 1.0, name=\"eta\"),\n",
    "        Real(0.01, 1.0, name=\"subsample\"),\n",
    "        Real(0.01, 1.0, name=\"colsample_bytree\"),\n",
    "        Integer(1, 10, name=\"min_child_weight\"),\n",
    "        Real(0.01, 10.0, name=\"lambda\"),\n",
    "        Real(0.01, 10.0, name=\"alpha\"),\n",
    "    ]\n",
    "\n",
    "    # Update default space with kwargs\n",
    "    updated_space = {**default_space, **kwargs}\n",
    "\n",
    "    # Convert to skopt space format\n",
    "    return [updated_space[key] for key in updated_space]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
